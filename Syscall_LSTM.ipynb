{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Syscall-LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMemh1nZnThnCpnfxjyMnX7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ndhpro/Colab_Notebooks/blob/master/Syscall_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkLTvDeB-8LO",
        "outputId": "0e3949d9-70b2-474b-b8c0-82b8270c4943"
      },
      "source": [
        "! git clone https://github.com/ndhpro/syscall-lstm.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'syscall-lstm'...\n",
            "remote: Enumerating objects: 738, done.\u001b[K\n",
            "remote: Counting objects: 100% (738/738), done.\u001b[K\n",
            "remote: Compressing objects: 100% (249/249), done.\u001b[K\n",
            "remote: Total 738 (delta 487), reused 735 (delta 484), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (738/738), 226.39 KiB | 4.44 MiB/s, done.\n",
            "Resolving deltas: 100% (487/487), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elaZMV4w_Cvg",
        "outputId": "60443615-288a-4afe-f359-3bce3b2881eb"
      },
      "source": [
        "cd syscall-lstm/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/syscall-lstm\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJLdUvSc_lPe"
      },
      "source": [
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "import numpy as np"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_J3mrD8Y_nLA",
        "outputId": "d12dea4f-b13b-4f1a-9456-196b51dd17e7"
      },
      "source": [
        "X, y = [], []\n",
        "paths = glob('data/*/*')\n",
        "for path in tqdm(paths, desc='Reading data'):\n",
        "    with open(path, 'r') as f:\n",
        "        seq = f.read()\n",
        "    X.append(seq)\n",
        "    if 'malware' in path:\n",
        "        y.append(1)\n",
        "    else:\n",
        "        y.append(0)\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, stratify=y, random_state=2020)\n",
        "X_mal = X_train[y_train == 1]\n",
        "X_beg = X_train[y_train == 0]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading data: 100%|██████████| 889/889 [00:00<00:00, 19480.26it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qxoe_36D_p0D"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "vocab = tokenizer.word_index\n",
        "vocab_size = len(vocab) + 1\n",
        "\n",
        "X_mal = tokenizer.texts_to_sequences(X_mal)\n",
        "X_beg = tokenizer.texts_to_sequences(X_beg)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNHvGBQH_t5S",
        "outputId": "8781a35a-0074-46ef-fc35-3300e87f86ab"
      },
      "source": [
        "def create_model():\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=1000, return_sequences=True,\n",
        "                   input_shape=(100, len(vocab)+1)))\n",
        "    # model.add(LSTM(units=1000, return_sequences=True))\n",
        "    # model.add(LSTM(units=1000, return_sequences=True))\n",
        "    model.add(LSTM(units=1000))\n",
        "    model.add(Dense(units=len(vocab)+1, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "malicious_model = create_model()\n",
        "trusted_model = create_model()\n",
        "print(malicious_model.summary())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_8 (LSTM)                (None, 100, 1000)         4552000   \n",
            "_________________________________________________________________\n",
            "lstm_9 (LSTM)                (None, 1000)              8004000   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 137)               137137    \n",
            "=================================================================\n",
            "Total params: 12,693,137\n",
            "Trainable params: 12,693,137\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHhFb_oG_2Wq"
      },
      "source": [
        "def prepare_sentence(seq):\n",
        "    # Pads seq and slides windows\n",
        "    x = []\n",
        "    y = []\n",
        "    for i in range(min(101, len(seq))):\n",
        "        x_padded = pad_sequences([seq[:i]], maxlen=100, padding='pre')[0]\n",
        "        x_onehot = [to_categorical(i, num_classes=vocab_size) for i in x_padded]\n",
        "        x.append(x_onehot)\n",
        "        y.append(to_categorical(seq[i], num_classes=vocab_size))\n",
        "    return x, y"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HR6qcWYvAGiC",
        "outputId": "1752c905-0957-4f6a-f203-8e106a84b8b5"
      },
      "source": [
        "# Pad sequences and slide windows\n",
        "x = []\n",
        "y = []\n",
        "for seq in X_mal:\n",
        "    x_windows, y_windows = prepare_sentence(seq)\n",
        "    x += x_windows[-10:]\n",
        "    y += y_windows[-10:]\n",
        "x = np.array(x)\n",
        "y = np.array(y)\n",
        "\n",
        "print(x.shape, y.shape)\n",
        "malicious_model.fit(x, y, epochs=50, batch_size=128, verbose=2)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4260, 100, 137) (4260, 137)\n",
            "Epoch 1/50\n",
            "34/34 - 6s - loss: 1.4696 - accuracy: 0.6721\n",
            "Epoch 2/50\n",
            "34/34 - 6s - loss: 0.9685 - accuracy: 0.6948\n",
            "Epoch 3/50\n",
            "34/34 - 6s - loss: 0.8731 - accuracy: 0.7272\n",
            "Epoch 4/50\n",
            "34/34 - 6s - loss: 0.8032 - accuracy: 0.7415\n",
            "Epoch 5/50\n",
            "34/34 - 6s - loss: 0.7341 - accuracy: 0.7711\n",
            "Epoch 6/50\n",
            "34/34 - 6s - loss: 0.6099 - accuracy: 0.8038\n",
            "Epoch 7/50\n",
            "34/34 - 6s - loss: 0.5321 - accuracy: 0.8171\n",
            "Epoch 8/50\n",
            "34/34 - 6s - loss: 0.4564 - accuracy: 0.8441\n",
            "Epoch 9/50\n",
            "34/34 - 6s - loss: 0.3968 - accuracy: 0.8765\n",
            "Epoch 10/50\n",
            "34/34 - 6s - loss: 0.3137 - accuracy: 0.9075\n",
            "Epoch 11/50\n",
            "34/34 - 6s - loss: 0.2566 - accuracy: 0.9237\n",
            "Epoch 12/50\n",
            "34/34 - 6s - loss: 0.2112 - accuracy: 0.9385\n",
            "Epoch 13/50\n",
            "34/34 - 6s - loss: 0.1779 - accuracy: 0.9448\n",
            "Epoch 14/50\n",
            "34/34 - 6s - loss: 0.1646 - accuracy: 0.9549\n",
            "Epoch 15/50\n",
            "34/34 - 6s - loss: 0.1351 - accuracy: 0.9622\n",
            "Epoch 16/50\n",
            "34/34 - 6s - loss: 0.1115 - accuracy: 0.9671\n",
            "Epoch 17/50\n",
            "34/34 - 6s - loss: 0.1007 - accuracy: 0.9714\n",
            "Epoch 18/50\n",
            "34/34 - 6s - loss: 0.0852 - accuracy: 0.9768\n",
            "Epoch 19/50\n",
            "34/34 - 6s - loss: 0.0785 - accuracy: 0.9808\n",
            "Epoch 20/50\n",
            "34/34 - 6s - loss: 0.0727 - accuracy: 0.9810\n",
            "Epoch 21/50\n",
            "34/34 - 6s - loss: 0.0711 - accuracy: 0.9815\n",
            "Epoch 22/50\n",
            "34/34 - 6s - loss: 0.0677 - accuracy: 0.9793\n",
            "Epoch 23/50\n",
            "34/34 - 6s - loss: 0.0558 - accuracy: 0.9854\n",
            "Epoch 24/50\n",
            "34/34 - 6s - loss: 0.0445 - accuracy: 0.9887\n",
            "Epoch 25/50\n",
            "34/34 - 6s - loss: 0.0404 - accuracy: 0.9894\n",
            "Epoch 26/50\n",
            "34/34 - 6s - loss: 0.0441 - accuracy: 0.9873\n",
            "Epoch 27/50\n",
            "34/34 - 6s - loss: 0.0349 - accuracy: 0.9894\n",
            "Epoch 28/50\n",
            "34/34 - 6s - loss: 0.0398 - accuracy: 0.9894\n",
            "Epoch 29/50\n",
            "34/34 - 6s - loss: 0.0319 - accuracy: 0.9906\n",
            "Epoch 30/50\n",
            "34/34 - 6s - loss: 0.0347 - accuracy: 0.9911\n",
            "Epoch 31/50\n",
            "34/34 - 6s - loss: 0.0407 - accuracy: 0.9894\n",
            "Epoch 32/50\n",
            "34/34 - 6s - loss: 0.0247 - accuracy: 0.9937\n",
            "Epoch 33/50\n",
            "34/34 - 6s - loss: 0.0229 - accuracy: 0.9951\n",
            "Epoch 34/50\n",
            "34/34 - 6s - loss: 0.0251 - accuracy: 0.9932\n",
            "Epoch 35/50\n",
            "34/34 - 6s - loss: 0.0265 - accuracy: 0.9927\n",
            "Epoch 36/50\n",
            "34/34 - 6s - loss: 0.0212 - accuracy: 0.9944\n",
            "Epoch 37/50\n",
            "34/34 - 6s - loss: 0.0247 - accuracy: 0.9932\n",
            "Epoch 38/50\n",
            "34/34 - 6s - loss: 0.0205 - accuracy: 0.9939\n",
            "Epoch 39/50\n",
            "34/34 - 6s - loss: 0.0214 - accuracy: 0.9927\n",
            "Epoch 40/50\n",
            "34/34 - 6s - loss: 0.0151 - accuracy: 0.9960\n",
            "Epoch 41/50\n",
            "34/34 - 6s - loss: 0.0209 - accuracy: 0.9946\n",
            "Epoch 42/50\n",
            "34/34 - 6s - loss: 0.0199 - accuracy: 0.9934\n",
            "Epoch 43/50\n",
            "34/34 - 6s - loss: 0.0215 - accuracy: 0.9951\n",
            "Epoch 44/50\n",
            "34/34 - 6s - loss: 0.0170 - accuracy: 0.9962\n",
            "Epoch 45/50\n",
            "34/34 - 6s - loss: 0.0163 - accuracy: 0.9953\n",
            "Epoch 46/50\n",
            "34/34 - 6s - loss: 0.0149 - accuracy: 0.9958\n",
            "Epoch 47/50\n",
            "34/34 - 6s - loss: 0.0154 - accuracy: 0.9962\n",
            "Epoch 48/50\n",
            "34/34 - 6s - loss: 0.0154 - accuracy: 0.9953\n",
            "Epoch 49/50\n",
            "34/34 - 6s - loss: 0.0132 - accuracy: 0.9962\n",
            "Epoch 50/50\n",
            "34/34 - 6s - loss: 0.0158 - accuracy: 0.9960\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd02f4e9cc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTBukAqoIbyL",
        "outputId": "2f9687f2-950f-4514-a68a-a9cc38cc3ad7"
      },
      "source": [
        "# Pad sequences and slide windows\n",
        "x = []\n",
        "y = []\n",
        "for seq in X_beg:\n",
        "    x_windows, y_windows = prepare_sentence(seq)\n",
        "    x += x_windows[-10:]\n",
        "    y += y_windows[-10:]\n",
        "x = np.array(x)\n",
        "y = np.array(y)\n",
        "\n",
        "print(x.shape, y.shape)\n",
        "trusted_model.fit(x, y, epochs=50, batch_size=128, verbose=2)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1960, 100, 137) (1960, 137)\n",
            "Epoch 1/50\n",
            "16/16 - 3s - loss: 3.5486 - accuracy: 0.2056\n",
            "Epoch 2/50\n",
            "16/16 - 3s - loss: 2.5735 - accuracy: 0.2495\n",
            "Epoch 3/50\n",
            "16/16 - 3s - loss: 2.3739 - accuracy: 0.2944\n",
            "Epoch 4/50\n",
            "16/16 - 3s - loss: 2.2654 - accuracy: 0.3102\n",
            "Epoch 5/50\n",
            "16/16 - 3s - loss: 2.2498 - accuracy: 0.3372\n",
            "Epoch 6/50\n",
            "16/16 - 3s - loss: 2.1692 - accuracy: 0.3306\n",
            "Epoch 7/50\n",
            "16/16 - 3s - loss: 1.9803 - accuracy: 0.3709\n",
            "Epoch 8/50\n",
            "16/16 - 3s - loss: 1.7773 - accuracy: 0.4612\n",
            "Epoch 9/50\n",
            "16/16 - 3s - loss: 1.5985 - accuracy: 0.5699\n",
            "Epoch 10/50\n",
            "16/16 - 3s - loss: 1.3428 - accuracy: 0.6286\n",
            "Epoch 11/50\n",
            "16/16 - 3s - loss: 1.2059 - accuracy: 0.6434\n",
            "Epoch 12/50\n",
            "16/16 - 3s - loss: 1.1170 - accuracy: 0.6689\n",
            "Epoch 13/50\n",
            "16/16 - 3s - loss: 1.0074 - accuracy: 0.7071\n",
            "Epoch 14/50\n",
            "16/16 - 3s - loss: 0.9556 - accuracy: 0.7015\n",
            "Epoch 15/50\n",
            "16/16 - 3s - loss: 0.8653 - accuracy: 0.7281\n",
            "Epoch 16/50\n",
            "16/16 - 3s - loss: 0.8174 - accuracy: 0.7342\n",
            "Epoch 17/50\n",
            "16/16 - 3s - loss: 0.8313 - accuracy: 0.7393\n",
            "Epoch 18/50\n",
            "16/16 - 3s - loss: 0.7689 - accuracy: 0.7367\n",
            "Epoch 19/50\n",
            "16/16 - 3s - loss: 1.0055 - accuracy: 0.7179\n",
            "Epoch 20/50\n",
            "16/16 - 3s - loss: 0.9283 - accuracy: 0.7199\n",
            "Epoch 21/50\n",
            "16/16 - 3s - loss: 0.7382 - accuracy: 0.7643\n",
            "Epoch 22/50\n",
            "16/16 - 3s - loss: 0.6446 - accuracy: 0.7872\n",
            "Epoch 23/50\n",
            "16/16 - 3s - loss: 0.6033 - accuracy: 0.7954\n",
            "Epoch 24/50\n",
            "16/16 - 3s - loss: 0.5623 - accuracy: 0.7954\n",
            "Epoch 25/50\n",
            "16/16 - 3s - loss: 0.4904 - accuracy: 0.8347\n",
            "Epoch 26/50\n",
            "16/16 - 3s - loss: 0.4390 - accuracy: 0.8449\n",
            "Epoch 27/50\n",
            "16/16 - 3s - loss: 0.3905 - accuracy: 0.8587\n",
            "Epoch 28/50\n",
            "16/16 - 3s - loss: 0.3350 - accuracy: 0.8878\n",
            "Epoch 29/50\n",
            "16/16 - 3s - loss: 0.3227 - accuracy: 0.8888\n",
            "Epoch 30/50\n",
            "16/16 - 3s - loss: 0.3020 - accuracy: 0.8949\n",
            "Epoch 31/50\n",
            "16/16 - 3s - loss: 0.2823 - accuracy: 0.9010\n",
            "Epoch 32/50\n",
            "16/16 - 3s - loss: 0.2810 - accuracy: 0.9041\n",
            "Epoch 33/50\n",
            "16/16 - 3s - loss: 0.2948 - accuracy: 0.8913\n",
            "Epoch 34/50\n",
            "16/16 - 3s - loss: 0.3469 - accuracy: 0.8821\n",
            "Epoch 35/50\n",
            "16/16 - 3s - loss: 0.3184 - accuracy: 0.8908\n",
            "Epoch 36/50\n",
            "16/16 - 3s - loss: 0.2695 - accuracy: 0.9087\n",
            "Epoch 37/50\n",
            "16/16 - 3s - loss: 0.2093 - accuracy: 0.9327\n",
            "Epoch 38/50\n",
            "16/16 - 3s - loss: 0.1836 - accuracy: 0.9454\n",
            "Epoch 39/50\n",
            "16/16 - 3s - loss: 0.1668 - accuracy: 0.9510\n",
            "Epoch 40/50\n",
            "16/16 - 3s - loss: 0.1635 - accuracy: 0.9505\n",
            "Epoch 41/50\n",
            "16/16 - 3s - loss: 0.1491 - accuracy: 0.9536\n",
            "Epoch 42/50\n",
            "16/16 - 3s - loss: 0.1596 - accuracy: 0.9403\n",
            "Epoch 43/50\n",
            "16/16 - 3s - loss: 0.1345 - accuracy: 0.9587\n",
            "Epoch 44/50\n",
            "16/16 - 3s - loss: 0.1326 - accuracy: 0.9612\n",
            "Epoch 45/50\n",
            "16/16 - 3s - loss: 0.1268 - accuracy: 0.9612\n",
            "Epoch 46/50\n",
            "16/16 - 3s - loss: 0.1050 - accuracy: 0.9658\n",
            "Epoch 47/50\n",
            "16/16 - 3s - loss: 0.0932 - accuracy: 0.9755\n",
            "Epoch 48/50\n",
            "16/16 - 3s - loss: 0.0767 - accuracy: 0.9770\n",
            "Epoch 49/50\n",
            "16/16 - 3s - loss: 0.1219 - accuracy: 0.9638\n",
            "Epoch 50/50\n",
            "16/16 - 3s - loss: 0.1163 - accuracy: 0.9668\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd032671a58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LgkSIUTJWkR"
      },
      "source": [
        "malicious_model.save_weights('model/malicious.h5')\n",
        "trusted_model.save_weights('model/trusted.h5')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFO9oBK6Jf4R",
        "outputId": "cb719182-8675-4aa3-9086-651247328833"
      },
      "source": [
        "vocab_inv = {v: k for k, v in tokenizer.word_index.items()}\n",
        "\n",
        "correct = 0\n",
        "for sentence, y_true in zip(X_test, y_test):\n",
        "  tok = tokenizer.texts_to_sequences([sentence])[0]\n",
        "  x_test, y = prepare_sentence(tok)\n",
        "  x_test = np.array(x_test)\n",
        "  y = np.array(y)\n",
        "\n",
        "  p_pred = malicious_model.predict(x_test)\n",
        "  log_p_sentence = 0\n",
        "  for i, prob in enumerate(p_pred):\n",
        "      prob_word = prob[np.argmax(y[i])]\n",
        "      log_p_sentence += np.log(prob_word)\n",
        "  mal_p = np.exp(log_p_sentence)\n",
        "\n",
        "  p_pred = trusted_model.predict(x_test)\n",
        "  log_p_sentence = 0\n",
        "  for i, prob in enumerate(p_pred):\n",
        "      prob_word = prob[np.argmax(y[i])]\n",
        "      log_p_sentence += np.log(prob_word)\n",
        "  beg_p = np.exp(log_p_sentence)\n",
        "\n",
        "  pred = 0 if beg_p > mal_p else 1\n",
        "  print(mal_p, beg_p, pred, y_true)\n",
        "  if pred == y_true:\n",
        "    correct += 1\n",
        "print('Accuracy:', correct / len(X_test))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0 3.1685352113609216e-13 0 0\n",
            "0.0 5.915102987707892e-43 0 0\n",
            "1.091018461794263e-189 3.3266092514091274e-279 1 0\n",
            "1.6713030172683961e-186 0.0 1 1\n",
            "1.7331120916559089e-150 0.0 1 1\n",
            "2.2844103314271426e-124 3.6805254968051538e-233 1 1\n",
            "7.797186314862153e-183 7.940347203479314e-122 0 1\n",
            "0.0 3.552869550046356e-284 0 0\n",
            "1.2405077283267185e-223 3.766531714320793e-182 0 0\n",
            "0.0 5.915102987707892e-43 0 0\n",
            "1.6738806850866854e-186 0.0 1 1\n",
            "2.418743272345645e-137 2.2253869990592885e-140 1 0\n",
            "1.6738806850866854e-186 0.0 1 1\n",
            "8.851983007758856e-155 0.0 1 1\n",
            "2.9464014623393895e-110 1.3314015614778075e-279 1 1\n",
            "2.9464014623393895e-110 1.3314015614778075e-279 1 1\n",
            "8.851983007758856e-155 0.0 1 1\n",
            "7.167243595154529e-103 2.1840159760194103e-269 1 1\n",
            "1.6738806850866854e-186 0.0 1 1\n",
            "1.0455707390813251e-176 1.8955884768529245e-247 1 1\n",
            "2.0933370202368114e-129 4.0003876988850045e-213 1 1\n",
            "0.0 2.476617462390339e-46 0 0\n",
            "2.2912894787250445e-175 1.2701715548288877e-286 1 1\n",
            "5.8760702835122874e-179 6.495067014e-315 1 1\n",
            "7.797186314862153e-183 7.940347203479314e-122 0 1\n",
            "8.962431946537127e-181 0.0 1 1\n",
            "1.046613622024111e-130 6.111681514596428e-254 1 1\n",
            "1.7331120916559089e-150 0.0 1 1\n",
            "0.0 2.293552218834338e-76 0 0\n",
            "0.0 1.2497547031592688e-228 0 0\n",
            "3.527428913703402e-142 1.70873039375e-312 1 1\n",
            "2.715370108813782e-54 3.93484742448364e-309 1 1\n",
            "3.4681728347863355e-129 7.356634494253378e-297 1 1\n",
            "5e-324 3.5684342920946024e-245 0 0\n",
            "7.501036140037598e-103 5.31411358635917e-114 1 1\n",
            "2.2844103314271426e-124 3.6805254968051538e-233 1 1\n",
            "1.0709829257800074e-110 1.2556349610506683e-291 1 1\n",
            "2.2771408153780794e-151 5.065186379568404e-260 1 1\n",
            "7.008840721030403e-114 3.2372233427458505e-175 1 1\n",
            "0.0 5.915102987707892e-43 0 0\n",
            "9.439470802986198e-83 0.0 1 1\n",
            "0.0 3.9883545666601054e-188 0 0\n",
            "5.136695902441684e-147 7.020442282508787e-218 1 1\n",
            "0.0 6.153291609624637e-65 0 0\n",
            "0.0 6.153291609624637e-65 0 0\n",
            "1.6738806850866854e-186 0.0 1 1\n",
            "0.0 9.664906676807075e-51 0 0\n",
            "8.962431946537127e-181 0.0 1 1\n",
            "1.8114113556237414e-188 0.0 1 1\n",
            "2.0115532218809382e-108 1.2459187355733544e-190 1 1\n",
            "8.962431946537127e-181 0.0 1 1\n",
            "2.715370108813782e-54 3.93484742448364e-309 1 1\n",
            "3.009240377451212e-130 9.884580245568947e-89 0 0\n",
            "1.6738806850866854e-186 0.0 1 1\n",
            "3.4681728347863355e-129 7.356634494253378e-297 1 1\n",
            "0.0 1.2668808704432019e-39 0 0\n",
            "1.0988890499169707e-82 9.186600712045131e-307 1 1\n",
            "0.0 8.57018787515107e-43 0 0\n",
            "0.0 2.1974548400628535e-97 0 0\n",
            "1.7331120916559089e-150 0.0 1 1\n",
            "1.7331120916559089e-150 0.0 1 1\n",
            "1.0709829257800074e-110 1.2556349610506683e-291 1 1\n",
            "2.715370108813782e-54 3.93484742448364e-309 1 1\n",
            "1.6738806850866854e-186 0.0 1 1\n",
            "1.0709829257800074e-110 1.2556349610506683e-291 1 1\n",
            "0.0 1.2668808704432019e-39 0 0\n",
            "2.9464014623393895e-110 1.3314015614778075e-279 1 1\n",
            "4.627075983626801e-85 0.0 1 1\n",
            "5.1748220787725913e-163 0.0 1 1\n",
            "7.624206200491918e-101 5.361703298217187e-288 1 1\n",
            "0.0 1.1173162563579315e-132 0 0\n",
            "1.6738806850866854e-186 0.0 1 1\n",
            "0.0 5.915102987707892e-43 0 0\n",
            "2.9464014623393895e-110 1.3314015614778075e-279 1 1\n",
            "1.6738806850866854e-186 0.0 1 1\n",
            "1.6738806850866854e-186 0.0 1 1\n",
            "2.715370108813782e-54 3.93484742448364e-309 1 1\n",
            "1.0709829257800074e-110 1.2556349610506683e-291 1 1\n",
            "2.66609985132365e-106 0.0 1 1\n",
            "2.7070187197347625e-57 7.935015392017506e-308 1 1\n",
            "1.0709829257800074e-110 1.2556349610506683e-291 1 1\n",
            "4.627075983626801e-85 0.0 1 1\n",
            "7.167243595154529e-103 2.1840159760194103e-269 1 1\n",
            "7.209999941606682e-284 4.083905173658384e-194 0 0\n",
            "1.0709829257800074e-110 1.2556349610506683e-291 1 1\n",
            "1.6313918602419942e-290 4.5312940295399735e-67 0 0\n",
            "0.0 3.1685352113609216e-13 0 0\n",
            "7.624206200491918e-101 5.361703298217187e-288 1 1\n",
            "2.4494606069643332e-112 6.896608904215361e-212 1 1\n",
            "2.6633995933581118e-139 2.5632455122831947e-298 1 1\n",
            "1.6738806850866854e-186 0.0 1 1\n",
            "1.1889871472512932e-292 0.0 1 1\n",
            "0.0 1.8672164150871423e-141 0 0\n",
            "0.0 3.1685352113609216e-13 0 0\n",
            "0.0 1.2668808704432019e-39 0 0\n",
            "3.484375700061694e-90 0.0 1 1\n",
            "3.9039611362005465e-181 0.0 1 1\n",
            "3.060618334276191e-50 6.14458305903378e-302 1 1\n",
            "1.057628422082053e-125 4.707788470092496e-292 1 1\n",
            "1.0709829257800074e-110 1.2556349610506683e-291 1 1\n",
            "1.6738806850866854e-186 0.0 1 1\n",
            "1.7314528205410358e-307 0.0 1 1\n",
            "1.3446933378144653e-126 2.78253e-319 1 1\n",
            "0.0 3.172110367843947e-67 0 0\n",
            "3.5066804207063605e-149 2.3877964102850954e-303 1 1\n",
            "0.0 5.915102987707892e-43 0 0\n",
            "0.0 5.052432913508063e-119 0 0\n",
            "0.0 9.88724164202021e-174 0 0\n",
            "1.6738806850866854e-186 0.0 1 1\n",
            "1.3568163964068898e-247 0.0 1 1\n",
            "0.0 6.153291609624637e-65 0 0\n",
            "2.715370108813782e-54 3.93484742448364e-309 1 1\n",
            "1.8000288757333773e-115 1.1878179005103355e-298 1 1\n",
            "1.0709829257800074e-110 1.2556349610506683e-291 1 1\n",
            "2.9235681753589155e-144 6.170345963123279e-263 1 1\n",
            "1.3568163964068898e-247 0.0 1 1\n",
            "0.0 1.5526019318253885e-36 0 0\n",
            "1.6738806850866854e-186 0.0 1 1\n",
            "8.962431946537127e-181 0.0 1 1\n",
            "8.867179822990604e-129 2.5982337445732584e-186 1 1\n",
            "0.0 5.597363139248824e-67 0 0\n",
            "2.7070187197347625e-57 7.935015392017506e-308 1 1\n",
            "9.708128416022558e-196 4.14528363754467e-127 0 0\n",
            "1.982303464644746e-48 4.09467962267e-313 1 1\n",
            "1.7331120916559089e-150 0.0 1 1\n",
            "1.4283543159451876e-116 7.08367755133284e-196 1 1\n",
            "1.0709829257800074e-110 1.2556349610506683e-291 1 1\n",
            "1.0709829257800074e-110 1.2556349610506683e-291 1 1\n",
            "7.167243595154529e-103 2.1840159760194103e-269 1 1\n",
            "0.0 5.915102987707892e-43 0 0\n",
            "1.6738806850866854e-186 0.0 1 1\n",
            "1.6738806850866854e-186 0.0 1 1\n",
            "0.0 5.915102987707892e-43 0 0\n",
            "8.3047495212946e-289 4.0467495526423296e-151 0 0\n",
            "2.648588738943028e-35 7.062874361499579e-289 1 1\n",
            "0.0 5.915102987707892e-43 0 0\n",
            "0.0 2.83388192655953e-41 0 0\n",
            "7.167243595154529e-103 2.1840159760194103e-269 1 1\n",
            "1.6738806850866854e-186 0.0 1 1\n",
            "1.2772496364960266e-29 8.886921023744157e-277 1 1\n",
            "4.627075983626801e-85 0.0 1 1\n",
            "0.0 5.915102987707892e-43 0 0\n",
            "1.057628422082053e-125 4.707788470092496e-292 1 1\n",
            "1.5382084979499e-143 7.82713559447807e-263 1 1\n",
            "0.0 5.052432913508063e-119 0 0\n",
            "8.286630391848156e-192 2.5729765402585594e-253 1 1\n",
            "8.962431946537127e-181 0.0 1 1\n",
            "4.516926834688363e-239 1.932609937038177e-203 0 1\n",
            "1.0709829257800074e-110 1.2556349610506683e-291 1 1\n",
            "1.1873021071812381e-137 9.59389819651812e-208 1 1\n",
            "0.0 2.091650551253214e-66 0 0\n",
            "1.301868845603792e-32 8.872053803198564e-295 1 1\n",
            "0.0 2.091650551253214e-66 0 0\n",
            "1.982303464644746e-48 4.09467962267e-313 1 0\n",
            "1.7331120916559089e-150 0.0 1 1\n",
            "2.715370108813782e-54 3.93484742448364e-309 1 1\n",
            "0.0 1.1666596343339401e-183 0 0\n",
            "2.9235681753589155e-144 6.170345963123279e-263 1 1\n",
            "0.0 8.57018787515107e-43 0 0\n",
            "1.0485304781740784e-189 0.0 1 1\n",
            "1.6738806850866854e-186 0.0 1 1\n",
            "0.0 5.7346331073249404e-189 0 0\n",
            "1.6738806850866854e-186 0.0 1 1\n",
            "1.0709829257800074e-110 1.2556349610506683e-291 1 1\n",
            "7.167243595154529e-103 2.1840159760194103e-269 1 1\n",
            "2.0115532218809382e-108 1.2459187355733544e-190 1 1\n",
            "0.0 1.0958173612752113e-200 0 0\n",
            "1.6738806850866854e-186 0.0 1 1\n",
            "0.0 6.975240846561929e-77 0 0\n",
            "0.0 5.915102987707892e-43 0 0\n",
            "1.982303464644746e-48 4.09467962267e-313 1 1\n",
            "5.5502397271486884e-101 1.5049645475647436e-287 1 1\n",
            "0.0 5.915102987707892e-43 0 0\n",
            "1.0709829257800074e-110 1.2556349610506683e-291 1 1\n",
            "2.2331178880098285e-130 1.7252513917200467e-284 1 1\n",
            "2.66609985132365e-106 0.0 1 1\n",
            "0.0 1.0617204763921037e-117 0 0\n",
            "1.5640939051275098e-188 2.6877867938397487e-270 1 1\n",
            "1.7331120916559089e-150 0.0 1 1\n",
            "3.5066804207063605e-149 2.3877964102850954e-303 1 1\n",
            "1.523918986849638e-120 4.138635827509346e-257 1 1\n",
            "5.960953134998498e-225 0.0 1 1\n",
            "1.441877031808944e-178 3.901628010479229e-251 1 1\n",
            "1.0709829257800074e-110 1.2556349610506683e-291 1 1\n",
            "1.1949385450496546e-123 7.947650395811354e-284 1 1\n",
            "0.0 3.96384172462453e-275 0 0\n",
            "7.167243595154529e-103 2.1840159760194103e-269 1 1\n",
            "8.039508912665548e-126 2.235561314213941e-190 1 1\n",
            "1.6738806850866854e-186 0.0 1 1\n",
            "1.6738806850866854e-186 0.0 1 1\n",
            "0.0 1.3662020716986066e-116 0 0\n",
            "0.0 5.915102987707892e-43 0 0\n",
            "1.6738806850866854e-186 0.0 1 1\n",
            "1.3479321560804119e-278 0.0 1 1\n",
            "1.3568163964068898e-247 0.0 1 1\n",
            "1.3756148760953104e-178 0.0 1 1\n",
            "1.6313918602419942e-290 4.5312940295399735e-67 0 0\n",
            "8.716630747492472e-109 6.6819209645024476e-285 1 1\n",
            "1.982303464644746e-48 4.09467962267e-313 1 1\n",
            "1.6313918602419942e-290 4.5312940295399735e-67 0 0\n",
            "1.6738806850866854e-186 0.0 1 1\n",
            "1.8268033364694954e-129 3.960541607015605e-201 1 1\n",
            "1.0709829257800074e-110 1.2556349610506683e-291 1 1\n",
            "1.6738806850866854e-186 0.0 1 1\n",
            "0.0 5.915102987707892e-43 0 0\n",
            "0.0 5.915102987707892e-43 0 0\n",
            "2.715370108813782e-54 3.93484742448364e-309 1 1\n",
            "4.273285556091382e-177 0.0 1 1\n",
            "7.76590281558075e-105 3.1520358515617975e-276 1 1\n",
            "1.745398701135811e-275 0.0 1 1\n",
            "0.0 4.802853819002732e-208 0 0\n",
            "1.6738806850866854e-186 0.0 1 1\n",
            "1.01786978366857e-149 1.7527539224240922e-240 1 1\n",
            "0.0 5.915102987707892e-43 0 0\n",
            "1.6738806850866854e-186 0.0 1 1\n",
            "0.0 5.915102987707892e-43 0 0\n",
            "2.5718078951280685e-113 1.6952658346863963e-272 1 1\n",
            "1.0485304781740784e-189 0.0 1 1\n",
            "2.2844103314271426e-124 3.6805254968051538e-233 1 1\n",
            "1.0709829257800074e-110 1.2556349610506683e-291 1 1\n",
            "0.0 5.915102987707892e-43 0 0\n",
            "0.0 5.915102987707892e-43 0 0\n",
            "0.0 1.2497547031592688e-228 0 0\n",
            "0.0 5.915102987707892e-43 0 0\n",
            "8.160820237306609e-180 8.928382027838966e-219 1 1\n",
            "0.0 5.915102987707892e-43 0 0\n",
            "7.209999941606682e-284 4.083905173658384e-194 0 0\n",
            "8.69862860645921e-184 6.788012504506331e-127 0 1\n",
            "0.0 2.7630861457990256e-48 0 0\n",
            "0.0 5.915102987707892e-43 0 0\n",
            "1.0709829257800074e-110 1.2556349610506683e-291 1 1\n",
            "0.0 2.7457475435283615e-94 0 0\n",
            "0.0 5.915102987707892e-43 0 0\n",
            "8.982876304747364e-53 8.895538e-317 1 1\n",
            "1.6738806850866854e-186 0.0 1 1\n",
            "1.6313918602419942e-290 4.5312940295399735e-67 0 0\n",
            "0.0 5.052432913508063e-119 0 0\n",
            "0.0 1.8672164150871423e-141 0 0\n",
            "1.3568163964068898e-247 0.0 1 1\n",
            "1.638720705535676e-153 6.662088330799962e-189 1 1\n",
            "7.624206200491918e-101 5.361703298217187e-288 1 1\n",
            "1.01786978366857e-149 1.7527539224240922e-240 1 1\n",
            "6.452749244725099e-79 5.383749993003e-312 1 1\n",
            "1.745398701135811e-275 0.0 1 1\n",
            "2.715370108813782e-54 3.93484742448364e-309 1 1\n",
            "0.0 1.1666596343339401e-183 0 0\n",
            "0.0 5.915102987707892e-43 0 0\n",
            "2.9464014623393895e-110 1.3314015614778075e-279 1 1\n",
            "8.851983007758856e-155 0.0 1 1\n",
            "2.9464014623393895e-110 1.3314015614778075e-279 1 1\n",
            "3.1612435434895975e-97 5.6950939163632674e-105 1 1\n",
            "2.715370108813782e-54 3.93484742448364e-309 1 1\n",
            "7.167243595154529e-103 2.1840159760194103e-269 1 1\n",
            "8.982876304747364e-53 8.895538e-317 1 1\n",
            "2.715370108813782e-54 3.93484742448364e-309 1 1\n",
            "1.0709829257800074e-110 1.2556349610506683e-291 1 1\n",
            "2.9464014623393895e-110 1.3314015614778075e-279 1 1\n",
            "1.6313918602419942e-290 4.5312940295399735e-67 0 0\n",
            "1.6313918602419942e-290 4.5312940295399735e-67 0 0\n",
            "1.6313918602419942e-290 4.5312940295399735e-67 0 0\n",
            "2.715370108813782e-54 3.93484742448364e-309 1 1\n",
            "7.624206200491918e-101 5.361703298217187e-288 1 1\n",
            "1.0709829257800074e-110 1.2556349610506683e-291 1 1\n",
            "0.0 3.884801965026421e-71 0 0\n",
            "1.0709829257800074e-110 1.2556349610506683e-291 1 1\n",
            "2.2086860776384377e-190 0.0 1 1\n",
            "4.627075983626801e-85 0.0 1 1\n",
            "Accuracy: 0.9737827715355806\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}